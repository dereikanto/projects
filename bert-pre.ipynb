{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.text import Tokenizer\n\ndata=pd.read_csv(\"/kaggle/input/tamil-troll/train_captions.csv\")\ndata.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-12T18:48:47.733937Z","iopub.execute_input":"2023-07-12T18:48:47.734276Z","iopub.status.idle":"2023-07-12T18:49:05.832192Z","shell.execute_reply.started":"2023-07-12T18:48:47.734251Z","shell.execute_reply":"2023-07-12T18:49:05.830494Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0           imagename  \\\n0           0     Not_troll_0.jpg   \n1           1     Not_troll_1.jpg   \n2           2    Not_troll_10.jpg   \n3           3   Not_troll_100.jpg   \n4           4  Not_troll_1000.jpg   \n\n                                           captions  \n0                             Ada pikkalipayalugala  \n1                 Etho sambavam nadandhirukkum pola  \n2             Vunnayellam frienda vechirukken paaru  \n3  Idho! Ivan dhaan naan nasama ponadhukku kaaranam  \n4  Ennada lusu thanama pesikittu irukka lusu payale  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>imagename</th>\n      <th>captions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Not_troll_0.jpg</td>\n      <td>Ada pikkalipayalugala</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Not_troll_1.jpg</td>\n      <td>Etho sambavam nadandhirukkum pola</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Not_troll_10.jpg</td>\n      <td>Vunnayellam frienda vechirukken paaru</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Not_troll_100.jpg</td>\n      <td>Idho! Ivan dhaan naan nasama ponadhukku kaaranam</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Not_troll_1000.jpg</td>\n      <td>Ennada lusu thanama pesikittu irukka lusu payale</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data2=pd.read_csv(\"/kaggle/input/tamil-troll/test_captions.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.834305Z","iopub.execute_input":"2023-07-12T18:49:05.835019Z","iopub.status.idle":"2023-07-12T18:49:05.853690Z","shell.execute_reply.started":"2023-07-12T18:49:05.834980Z","shell.execute_reply":"2023-07-12T18:49:05.852452Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# setting labels\ndef labeling(train):\n  train['Label'] = train['imagename'].str.replace('\\d+', '')\n  train['Label'] = train['Label'].str.replace('_.jpg', '')\n  train['Label'] = train['Label'].str.replace('_.png', '')\n  return train['Label']\n\ndata['Label']= labeling(data)\n\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.855565Z","iopub.execute_input":"2023-07-12T18:49:05.856417Z","iopub.status.idle":"2023-07-12T18:49:05.918406Z","shell.execute_reply.started":"2023-07-12T18:49:05.856382Z","shell.execute_reply":"2023-07-12T18:49:05.917457Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/2200995685.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  train['Label'] = train['imagename'].str.replace('\\d+', '')\n/tmp/ipykernel_28/2200995685.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n  train['Label'] = train['Label'].str.replace('_.jpg', '')\n/tmp/ipykernel_28/2200995685.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  train['Label'] = train['Label'].str.replace('_.png', '')\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0       imagename  \\\n0           0  test_img_0.jpg   \n1           1  test_img_1.jpg   \n2           2  test_img_2.jpg   \n3           3  test_img_3.jpg   \n4           4  test_img_4.jpg   \n\n                                            captions  label  \n0  sugarkaga nadandhava vida figuregaga nadandhav...  troll  \n1  i have come for my stones      stones thaane.....  troll  \n2  \"special porotta\" nu pottuierukke spacial kum ...  troll  \n3  *we : amma .. cooker 3 whistle vanthuchu off p...  troll  \n4  creating whatsapp group - 1st day vaanga ji.. ...  troll  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>imagename</th>\n      <th>captions</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test_img_0.jpg</td>\n      <td>sugarkaga nadandhava vida figuregaga nadandhav...</td>\n      <td>troll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test_img_1.jpg</td>\n      <td>i have come for my stones      stones thaane.....</td>\n      <td>troll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test_img_2.jpg</td>\n      <td>\"special porotta\" nu pottuierukke spacial kum ...</td>\n      <td>troll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>test_img_3.jpg</td>\n      <td>*we : amma .. cooker 3 whistle vanthuchu off p...</td>\n      <td>troll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>test_img_4.jpg</td>\n      <td>creating whatsapp group - 1st day vaanga ji.. ...</td>\n      <td>troll</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data2['label']=data2['label'].apply(lambda x: 1 if x=='not_troll' else 0)\ndata2","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.921192Z","iopub.execute_input":"2023-07-12T18:49:05.921784Z","iopub.status.idle":"2023-07-12T18:49:05.939601Z","shell.execute_reply.started":"2023-07-12T18:49:05.921732Z","shell.execute_reply":"2023-07-12T18:49:05.938664Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0         imagename  \\\n0             0    test_img_0.jpg   \n1             1    test_img_1.jpg   \n2             2    test_img_2.jpg   \n3             3    test_img_3.jpg   \n4             4    test_img_4.jpg   \n..          ...               ...   \n662         662  test_img_662.jpg   \n663         663  test_img_663.jpg   \n664         664  test_img_664.jpg   \n665         665  test_img_665.jpg   \n666         666  test_img_666.jpg   \n\n                                              captions  label  \n0    sugarkaga nadandhava vida figuregaga nadandhav...      0  \n1    i have come for my stones      stones thaane.....      0  \n2    \"special porotta\" nu pottuierukke spacial kum ...      0  \n3    *we : amma .. cooker 3 whistle vanthuchu off p...      0  \n4    creating whatsapp group - 1st day vaanga ji.. ...      0  \n..                                                 ...    ...  \n662  DURING SCHOOL DAYS  ME & FRNDS: ANNA MAIL CHEC...      0  \n663         REMEMBER THIS KID    THIS KID IS RIGHT NOW      0  \n664  *SINGLES: IVAN VERA ENGALA ROMBA TORCHER PANDR...      0  \n665  ENAIKI PUBG LA CHICKEN DINNER VANGURA PAYALUGA...      1  \n666                                     LATER THAT DAY      1  \n\n[667 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>imagename</th>\n      <th>captions</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test_img_0.jpg</td>\n      <td>sugarkaga nadandhava vida figuregaga nadandhav...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test_img_1.jpg</td>\n      <td>i have come for my stones      stones thaane.....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test_img_2.jpg</td>\n      <td>\"special porotta\" nu pottuierukke spacial kum ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>test_img_3.jpg</td>\n      <td>*we : amma .. cooker 3 whistle vanthuchu off p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>test_img_4.jpg</td>\n      <td>creating whatsapp group - 1st day vaanga ji.. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>662</th>\n      <td>662</td>\n      <td>test_img_662.jpg</td>\n      <td>DURING SCHOOL DAYS  ME &amp; FRNDS: ANNA MAIL CHEC...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>663</th>\n      <td>663</td>\n      <td>test_img_663.jpg</td>\n      <td>REMEMBER THIS KID    THIS KID IS RIGHT NOW</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>664</th>\n      <td>664</td>\n      <td>test_img_664.jpg</td>\n      <td>*SINGLES: IVAN VERA ENGALA ROMBA TORCHER PANDR...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>665</th>\n      <td>665</td>\n      <td>test_img_665.jpg</td>\n      <td>ENAIKI PUBG LA CHICKEN DINNER VANGURA PAYALUGA...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>666</th>\n      <td>666</td>\n      <td>test_img_666.jpg</td>\n      <td>LATER THAT DAY</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>667 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nEncoder = LabelEncoder()\n\n# function for encoding labels\ndef encodelabels(labels):  \n  labels =  Encoder.fit_transform(labels)                                                      \n  return labels\n\ndata['Label'] = encodelabels(data['Label'])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.941054Z","iopub.execute_input":"2023-07-12T18:49:05.941581Z","iopub.status.idle":"2023-07-12T18:49:05.949546Z","shell.execute_reply.started":"2023-07-12T18:49:05.941550Z","shell.execute_reply":"2023-07-12T18:49:05.948517Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data2.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.950919Z","iopub.execute_input":"2023-07-12T18:49:05.951546Z","iopub.status.idle":"2023-07-12T18:49:05.969650Z","shell.execute_reply.started":"2023-07-12T18:49:05.951513Z","shell.execute_reply":"2023-07-12T18:49:05.968801Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0       imagename  \\\n0           0  test_img_0.jpg   \n1           1  test_img_1.jpg   \n2           2  test_img_2.jpg   \n3           3  test_img_3.jpg   \n4           4  test_img_4.jpg   \n5           5  test_img_5.jpg   \n6           6  test_img_6.jpg   \n7           7  test_img_7.jpg   \n8           8  test_img_8.jpg   \n9           9  test_img_9.jpg   \n\n                                            captions  label  \n0  sugarkaga nadandhava vida figuregaga nadandhav...      0  \n1  i have come for my stones      stones thaane.....      0  \n2  \"special porotta\" nu pottuierukke spacial kum ...      0  \n3  *we : amma .. cooker 3 whistle vanthuchu off p...      0  \n4  creating whatsapp group - 1st day vaanga ji.. ...      0  \n5  *FOODIE : YAARUM PAARKKAMAL ENNAI PAARKIREN EN...      0  \n6                MARGALI MAADHA EFFECT YENNA KULLURU      0  \n7         YENNAMMA FEEL PANNI YELUTHI ERUKRANDA AVAN      0  \n8  TAMIL SERIALS BE LIKE... INI AVARUKKU BADHIL.....      1  \n9                     Mama ponnu Expectation reality      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>imagename</th>\n      <th>captions</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test_img_0.jpg</td>\n      <td>sugarkaga nadandhava vida figuregaga nadandhav...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test_img_1.jpg</td>\n      <td>i have come for my stones      stones thaane.....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test_img_2.jpg</td>\n      <td>\"special porotta\" nu pottuierukke spacial kum ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>test_img_3.jpg</td>\n      <td>*we : amma .. cooker 3 whistle vanthuchu off p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>test_img_4.jpg</td>\n      <td>creating whatsapp group - 1st day vaanga ji.. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>test_img_5.jpg</td>\n      <td>*FOODIE : YAARUM PAARKKAMAL ENNAI PAARKIREN EN...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>test_img_6.jpg</td>\n      <td>MARGALI MAADHA EFFECT YENNA KULLURU</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>test_img_7.jpg</td>\n      <td>YENNAMMA FEEL PANNI YELUTHI ERUKRANDA AVAN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>test_img_8.jpg</td>\n      <td>TAMIL SERIALS BE LIKE... INI AVARUKKU BADHIL.....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>test_img_9.jpg</td>\n      <td>Mama ponnu Expectation reality</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['Label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.970985Z","iopub.execute_input":"2023-07-12T18:49:05.971524Z","iopub.status.idle":"2023-07-12T18:49:05.993061Z","shell.execute_reply.started":"2023-07-12T18:49:05.971494Z","shell.execute_reply":"2023-07-12T18:49:05.991915Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1    1282\n0    1018\nName: Label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:05.997560Z","iopub.execute_input":"2023-07-12T18:49:05.998146Z","iopub.status.idle":"2023-07-12T18:49:07.003904Z","shell.execute_reply.started":"2023-07-12T18:49:05.998115Z","shell.execute_reply":"2023-07-12T18:49:07.002951Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nbert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:07.008452Z","iopub.execute_input":"2023-07-12T18:49:07.010708Z","iopub.status.idle":"2023-07-12T18:49:32.620352Z","shell.execute_reply.started":"2023-07-12T18:49:07.010673Z","shell.execute_reply":"2023-07-12T18:49:32.619341Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # Bert layers\n# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n# preprocessed_text = bert_preprocess(text_input)\n# outputs = bert_encoder(preprocessed_text)\n\n# # Extract the intermediate layer representation\n# intermediate_layer_output = outputs['pooled_output']\n\n# # Create a new model to get the feature vectors\n# feature_extraction_model = tf.keras.Model(inputs=text_input, outputs=intermediate_layer_output)\n\n# # Use the feature extraction model to get the feature vectors\n# bert1 = feature_extraction_model.predict(data['captions'])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:32.625115Z","iopub.execute_input":"2023-07-12T18:49:32.625400Z","iopub.status.idle":"2023-07-12T18:49:32.629814Z","shell.execute_reply.started":"2023-07-12T18:49:32.625376Z","shell.execute_reply":"2023-07-12T18:49:32.628910Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# # Print the shape of the feature vectors\n# print(\"Shape of feature vectors:\", bert1.shape)\n\n# # Print the length of the feature vectors\n# print(\"Length of feature vectors:\", len(bert1))\n\n# # Print the first 10 values of the feature vectors\n# print(\"First 1 values of feature vectors:\")\n# print(bert1[:1])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:32.631007Z","iopub.execute_input":"2023-07-12T18:49:32.631726Z","iopub.status.idle":"2023-07-12T18:49:33.326944Z","shell.execute_reply.started":"2023-07-12T18:49:32.631695Z","shell.execute_reply":"2023-07-12T18:49:33.325815Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1024, activation='relu', name=\"feature\")(l)\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:33.328737Z","iopub.execute_input":"2023-07-12T18:49:33.329477Z","iopub.status.idle":"2023-07-12T18:49:34.175791Z","shell.execute_reply.started":"2023-07-12T18:49:33.329422Z","shell.execute_reply":"2023-07-12T18:49:34.174756Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall')\n]\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=METRICS)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:34.177228Z","iopub.execute_input":"2023-07-12T18:49:34.177577Z","iopub.status.idle":"2023-07-12T18:49:34.205969Z","shell.execute_reply.started":"2023-07-12T18:49:34.177544Z","shell.execute_reply":"2023-07-12T18:49:34.205102Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.fit(data['captions'], data['Label'], epochs=10)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:49:34.207395Z","iopub.execute_input":"2023-07-12T18:49:34.207767Z","iopub.status.idle":"2023-07-12T18:53:55.349374Z","shell.execute_reply.started":"2023-07-12T18:49:34.207721Z","shell.execute_reply":"2023-07-12T18:53:55.348324Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/10\n72/72 [==============================] - 38s 316ms/step - loss: 0.6201 - accuracy: 0.7200 - precision: 0.7406 - recall: 0.7660\nEpoch 2/10\n72/72 [==============================] - 24s 327ms/step - loss: 0.4848 - accuracy: 0.7722 - precision: 0.7929 - recall: 0.8003\nEpoch 3/10\n72/72 [==============================] - 24s 334ms/step - loss: 0.4577 - accuracy: 0.7843 - precision: 0.8023 - recall: 0.8136\nEpoch 4/10\n72/72 [==============================] - 25s 348ms/step - loss: 0.4197 - accuracy: 0.8117 - precision: 0.8361 - recall: 0.8237\nEpoch 5/10\n72/72 [==============================] - 25s 353ms/step - loss: 0.4222 - accuracy: 0.8122 - precision: 0.8269 - recall: 0.8385\nEpoch 6/10\n72/72 [==============================] - 25s 348ms/step - loss: 0.4258 - accuracy: 0.8052 - precision: 0.8263 - recall: 0.8237\nEpoch 7/10\n72/72 [==============================] - 25s 348ms/step - loss: 0.4327 - accuracy: 0.8061 - precision: 0.8261 - recall: 0.8261\nEpoch 8/10\n72/72 [==============================] - 25s 348ms/step - loss: 0.4295 - accuracy: 0.8061 - precision: 0.8240 - recall: 0.8292\nEpoch 9/10\n72/72 [==============================] - 25s 348ms/step - loss: 0.4273 - accuracy: 0.8039 - precision: 0.8218 - recall: 0.8276\nEpoch 10/10\n72/72 [==============================] - 25s 345ms/step - loss: 0.3941 - accuracy: 0.8287 - precision: 0.8474 - recall: 0.8448\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fa6fee3fa90>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the test data\nresults = model.evaluate(data2['captions'], data2['label'])\n\n# Extract the evaluation metrics\naccuracy = results[model.metrics_names.index('accuracy')]\nprecision = results[model.metrics_names.index('precision')]\nrecall = results[model.metrics_names.index('recall')]\n\n# Calculate the F1 score\nf1_score = 2 * (precision * recall) / (precision + recall)\n\n# Print the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:53:55.352837Z","iopub.execute_input":"2023-07-12T18:53:55.353174Z","iopub.status.idle":"2023-07-12T18:54:06.586465Z","shell.execute_reply.started":"2023-07-12T18:53:55.353147Z","shell.execute_reply":"2023-07-12T18:54:06.585439Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"21/21 [==============================] - 8s 352ms/step - loss: 1.4630 - accuracy: 0.4198 - precision: 0.3964 - recall: 0.8088\nAccuracy: 0.419790118932724\nPrecision: 0.3963963985443115\nRecall: 0.8088235259056091\nF1 Score: 0.532043532010522\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Make predictions on the test data\npredictions = model.predict(data2['captions'])\npredicted_labels = np.argmax(predictions, axis=1)\n\n# Obtain true labels\ntrue_labels = data2['label']\n\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels)\n\n# Print the classification report\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:54:06.588044Z","iopub.execute_input":"2023-07-12T18:54:06.588416Z","iopub.status.idle":"2023-07-12T18:54:14.634317Z","shell.execute_reply.started":"2023-07-12T18:54:06.588374Z","shell.execute_reply":"2023-07-12T18:54:14.633319Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"21/21 [==============================] - 8s 344ms/step\n              precision    recall  f1-score   support\n\n           0       0.59      1.00      0.74       395\n           1       0.00      0.00      0.00       272\n\n    accuracy                           0.59       667\n   macro avg       0.30      0.50      0.37       667\nweighted avg       0.35      0.59      0.44       667\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract the feature vectors from the trained model\nfeature_extraction_model = tf.keras.Model(inputs=model.input,\n                                          outputs=model.get_layer('feature').output)\n\n# Get the feature vectors for your input data\nbert2 = feature_extraction_model.predict(data['captions'])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:15:50.378182Z","iopub.execute_input":"2023-07-10T13:15:50.378834Z","iopub.status.idle":"2023-07-10T13:16:16.014851Z","shell.execute_reply.started":"2023-07-10T13:15:50.378795Z","shell.execute_reply":"2023-07-10T13:16:16.013785Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"72/72 [==============================] - 25s 345ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the shape of the feature vectors\nprint(\"Shape of feature vectors:\", bert2.shape)\n\n# Print the length of the feature vectors\nprint(\"Length of feature vectors:\", len(bert2))\n\n# Print the first 10 values of the feature vectors\nprint(\"First 10 values of feature vectors:\")\nprint(bert2[:1])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:16:16.017329Z","iopub.execute_input":"2023-07-10T13:16:16.018069Z","iopub.status.idle":"2023-07-10T13:16:16.025881Z","shell.execute_reply.started":"2023-07-10T13:16:16.018028Z","shell.execute_reply":"2023-07-10T13:16:16.024571Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Shape of feature vectors: (2300, 1024)\nLength of feature vectors: 2300\nFirst 10 values of feature vectors:\n[[0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1024, activation='relu', name=\"feature\")(l)\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:23:41.939855Z","iopub.execute_input":"2023-07-10T13:23:41.940233Z","iopub.status.idle":"2023-07-10T13:23:42.105525Z","shell.execute_reply.started":"2023-07-10T13:23:41.940180Z","shell.execute_reply":"2023-07-10T13:23:42.104498Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall')\n]\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=METRICS)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:23:43.070750Z","iopub.execute_input":"2023-07-10T13:23:43.071147Z","iopub.status.idle":"2023-07-10T13:23:43.103678Z","shell.execute_reply.started":"2023-07-10T13:23:43.071116Z","shell.execute_reply":"2023-07-10T13:23:43.102630Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model.fit(data2['captions'], data2['label'], epochs=10)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:23:44.069233Z","iopub.execute_input":"2023-07-10T13:23:44.069614Z","iopub.status.idle":"2023-07-10T13:25:08.788740Z","shell.execute_reply.started":"2023-07-10T13:23:44.069583Z","shell.execute_reply":"2023-07-10T13:25:08.787643Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch 1/10\n21/21 [==============================] - 10s 344ms/step - loss: 1.2190 - accuracy: 0.4978 - precision: 0.3507 - recall: 0.2721\nEpoch 2/10\n21/21 [==============================] - 7s 354ms/step - loss: 0.7177 - accuracy: 0.5337 - precision: 0.4163 - recall: 0.3566\nEpoch 3/10\n21/21 [==============================] - 8s 372ms/step - loss: 0.6947 - accuracy: 0.5652 - precision: 0.4541 - recall: 0.3272\nEpoch 4/10\n21/21 [==============================] - 8s 366ms/step - loss: 0.6738 - accuracy: 0.5967 - precision: 0.5118 - recall: 0.2390\nEpoch 5/10\n21/21 [==============================] - 8s 361ms/step - loss: 0.7069 - accuracy: 0.5502 - precision: 0.4417 - recall: 0.3897\nEpoch 6/10\n21/21 [==============================] - 7s 353ms/step - loss: 0.6840 - accuracy: 0.5982 - precision: 0.5192 - recall: 0.1985\nEpoch 7/10\n21/21 [==============================] - 8s 360ms/step - loss: 0.7061 - accuracy: 0.5847 - precision: 0.4850 - recall: 0.2978\nEpoch 8/10\n21/21 [==============================] - 7s 346ms/step - loss: 0.6972 - accuracy: 0.5532 - precision: 0.4508 - recall: 0.4375\nEpoch 9/10\n21/21 [==============================] - 7s 342ms/step - loss: 0.6728 - accuracy: 0.6177 - precision: 0.5977 - recall: 0.1912\nEpoch 10/10\n21/21 [==============================] - 7s 342ms/step - loss: 0.6599 - accuracy: 0.6207 - precision: 0.6044 - recall: 0.2022\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7941f46df760>"},"metadata":{}}]},{"cell_type":"code","source":"# Extract the feature vectors from the trained model\nfeature_extraction_model = tf.keras.Model(inputs=model.input,\n                                          outputs=model.get_layer('feature').output)\n\n# Get the feature vectors for your input data\nbert_test = feature_extraction_model.predict(data2['captions'])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:25:13.015313Z","iopub.execute_input":"2023-07-10T13:25:13.015698Z","iopub.status.idle":"2023-07-10T13:25:24.035244Z","shell.execute_reply.started":"2023-07-10T13:25:13.015669Z","shell.execute_reply":"2023-07-10T13:25:24.034255Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"21/21 [==============================] - 8s 333ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the shape of the feature vectors\nprint(\"Shape of feature vectors:\", bert_test.shape)\n\n# Print the length of the feature vectors\nprint(\"Length of feature vectors:\", len(bert_test))\n\n# Print the first 10 values of the feature vectors\nprint(\"First 10 values of feature vectors:\")\nprint(bert_test[:1])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:25:24.037264Z","iopub.execute_input":"2023-07-10T13:25:24.037700Z","iopub.status.idle":"2023-07-10T13:25:24.045680Z","shell.execute_reply.started":"2023-07-10T13:25:24.037666Z","shell.execute_reply":"2023-07-10T13:25:24.043500Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Shape of feature vectors: (667, 1024)\nLength of feature vectors: 667\nFirst 10 values of feature vectors:\n[[0.37655833 0.         0.         ... 0.63003796 0.         0.        ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.text import Tokenizer\n# Setting seed for reproducibiltiy\nSEED = 42\nkeras.utils.set_random_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:40:42.884282Z","iopub.execute_input":"2023-07-12T15:40:42.884678Z","iopub.status.idle":"2023-07-12T15:40:42.934645Z","shell.execute_reply.started":"2023-07-12T15:40:42.884647Z","shell.execute_reply":"2023-07-12T15:40:42.933582Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# DATA\nBUFFER_SIZE = 512\nBATCH_SIZE = 256\n\n# AUGMENTATION\nIMAGE_SIZE=72\nPATCH_SIZE = 6\nNUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n\n# OPTIMIZER\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0001\n\n# TRAINING\nEPOCHS = 10\n\n# ARCHITECTURE\nLAYER_NORM_EPS = 1e-6\nTRANSFORMER_LAYERS = 8\nPROJECTION_DIM = 64\nNUM_HEADS = 4\nTRANSFORMER_UNITS = [\n    PROJECTION_DIM * 2,\n    PROJECTION_DIM,\n]\nMLP_HEAD_UNITS = [2048, 1024]","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:40:43.524099Z","iopub.execute_input":"2023-07-12T15:40:43.524468Z","iopub.status.idle":"2023-07-12T15:40:43.530927Z","shell.execute_reply.started":"2023-07-12T15:40:43.524439Z","shell.execute_reply":"2023-07-12T15:40:43.529975Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# training input in array\nimg_size = 72\ntrain_image_dir = '/kaggle/input/tamil-troll/uploaded_tamil_memes 1/uploaded_tamil_memes 1'\n# train_image_array = create_dataset(train_image_dir)\n# train_image_array.shape\n\ntrain_image_files = os.listdir(train_image_dir)\n\n\n# Create an empty array to store the images\ntrain_images_array = np.zeros((len(train_image_files), img_size, img_size, 3), dtype=np.uint8)\n\n\n\n# Loop through the training images and load them into the array\nfor i, file in enumerate(train_image_files):\n    img = cv2.imread(os.path.join(train_image_dir, file))\n    img = cv2.resize(img, (img_size, img_size))\n    train_images_array[i] = img","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:40:44.073352Z","iopub.execute_input":"2023-07-12T15:40:44.073725Z","iopub.status.idle":"2023-07-12T15:41:32.676037Z","shell.execute_reply.started":"2023-07-12T15:40:44.073695Z","shell.execute_reply":"2023-07-12T15:41:32.675027Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# training input in array\n# img_size = 224\ntest_image_dir = '/kaggle/input/tamil-troll/uploaded_tamil_memes 1/test_img'\n# train_image_array = create_dataset(train_image_dir)\n# train_image_array.shape\n\ntest_image_files = os.listdir(test_image_dir)\n\n\n# Create an empty array to store the images\ntest_images_array = np.zeros((len(test_image_files), img_size, img_size, 3), dtype=np.uint8)\n\n\n\n# Loop through the training images and load them into the array\nfor i, file in enumerate(test_image_files):\n    img = cv2.imread(os.path.join(test_image_dir, file))\n    img = cv2.resize(img, (img_size, img_size))\n    test_images_array[i] = img","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:32.679120Z","iopub.execute_input":"2023-07-12T15:41:32.679825Z","iopub.status.idle":"2023-07-12T15:41:42.718311Z","shell.execute_reply.started":"2023-07-12T15:41:32.679789Z","shell.execute_reply":"2023-07-12T15:41:42.717348Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(len(train_images_array))\nprint(len(test_images_array))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:42.719901Z","iopub.execute_input":"2023-07-12T15:41:42.720278Z","iopub.status.idle":"2023-07-12T15:41:42.726800Z","shell.execute_reply.started":"2023-07-12T15:41:42.720243Z","shell.execute_reply":"2023-07-12T15:41:42.725747Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"2300\n667\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train=train_images_array\nx_test=test_images_array\ny_train=data['Label']\ny_test=data2['label']","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:42.729591Z","iopub.execute_input":"2023-07-12T15:41:42.730014Z","iopub.status.idle":"2023-07-12T15:41:42.737966Z","shell.execute_reply.started":"2023-07-12T15:41:42.729981Z","shell.execute_reply":"2023-07-12T15:41:42.735540Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:42.739587Z","iopub.execute_input":"2023-07-12T15:41:42.739944Z","iopub.status.idle":"2023-07-12T15:41:42.755789Z","shell.execute_reply.started":"2023-07-12T15:41:42.739912Z","shell.execute_reply":"2023-07-12T15:41:42.754529Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([[[[  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1],\n         ...,\n         [  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        ...,\n\n        [[  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1],\n         ...,\n         [  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1]],\n\n        [[  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1],\n         ...,\n         [  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1]],\n\n        [[  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1],\n         ...,\n         [  1,   1,   1],\n         [  1,   1,   1],\n         [  1,   1,   1]]],\n\n\n       [[[133, 194, 243],\n         [160, 204, 232],\n         [164, 203, 231],\n         ...,\n         [156, 202, 233],\n         [168, 206, 229],\n         [ 85, 156, 230]],\n\n        [[200, 239, 253],\n         [239, 253, 251],\n         [246, 253, 249],\n         ...,\n         [235, 253, 244],\n         [239, 252, 236],\n         [115, 170, 207]],\n\n        [[189, 231, 250],\n         [240, 253, 251],\n         [247, 252, 250],\n         ...,\n         [240, 252, 249],\n         [241, 252, 241],\n         [110, 166, 208]],\n\n        ...,\n\n        [[198, 233, 250],\n         [242, 254, 248],\n         [247, 254, 246],\n         ...,\n         [246, 252, 250],\n         [243, 254, 243],\n         [123, 168, 212]],\n\n        [[182, 222, 252],\n         [226, 244, 249],\n         [229, 245, 247],\n         ...,\n         [221, 245, 248],\n         [233, 254, 249],\n         [109, 159, 208]],\n\n        [[ 61, 134, 215],\n         [ 69, 133, 208],\n         [ 71, 133, 207],\n         ...,\n         [ 67, 137, 203],\n         [ 73, 140, 208],\n         [ 62, 136, 222]]],\n\n\n       [[[  2,   1,   2],\n         [  2,   2,   2],\n         [  2,   2,   2],\n         ...,\n         [  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2]],\n\n        [[  0,   2,   2],\n         [  2,   3,   1],\n         [  3,   1,   2],\n         ...,\n         [  2,   2,   2],\n         [  2,   3,   1],\n         [  1,   1,   1]],\n\n        [[  0,   2,   3],\n         [  0,   4,   2],\n         [  0,   6,   0],\n         ...,\n         [  3,   4,   0],\n         [  5,   4,   3],\n         [  2,   2,   2]],\n\n        ...,\n\n        [[  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2],\n         ...,\n         [  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2]],\n\n        [[  2,   2,   2],\n         [  2,   2,   2],\n         [  1,   1,   1],\n         ...,\n         [  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2]],\n\n        [[  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2],\n         ...,\n         [  2,   2,   2],\n         [  2,   2,   2],\n         [  2,   2,   2]]],\n\n\n       ...,\n\n\n       [[[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        ...,\n\n        [[  3,   1,   1],\n         [ 11,   4,   5],\n         [  9,  11,  10],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  4,   0,   0],\n         [  1,   1,   1],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]]],\n\n\n       [[[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        ...,\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]],\n\n        [[  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0],\n         ...,\n         [  0,   0,   0],\n         [  0,   0,   0],\n         [  0,   0,   0]]],\n\n\n       [[[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]],\n\n        [[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]],\n\n        [[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]],\n\n        ...,\n\n        [[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]],\n\n        [[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]],\n\n        [[246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247],\n         ...,\n         [246, 252, 247],\n         [246, 252, 247],\n         [246, 252, 247]]]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(72, 72),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.02),\n        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(x_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:42.757552Z","iopub.execute_input":"2023-07-12T15:41:42.758131Z","iopub.status.idle":"2023-07-12T15:41:43.221931Z","shell.execute_reply.started":"2023-07-12T15:41:42.758094Z","shell.execute_reply":"2023-07-12T15:41:43.220908Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a batch of images x_train with shape (batch_size, height, width, channels)\n\n# Pass the sample batch through the data_augmentation model\naugmented_images = data_augmentation.predict(x_train)\n\n# Check the shape of augmented_images\nnum_images_after_augmentation = augmented_images.shape[0]\nprint(\"Number of images after augmentation:\", num_images_after_augmentation)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:41:43.223635Z","iopub.execute_input":"2023-07-12T15:41:43.223976Z","iopub.status.idle":"2023-07-12T15:41:44.058696Z","shell.execute_reply.started":"2023-07-12T15:41:43.223944Z","shell.execute_reply":"2023-07-12T15:41:44.057670Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"72/72 [==============================] - 0s 2ms/step\nNumber of images after augmentation: 2300\n","output_type":"stream"}]},{"cell_type":"code","source":"class ShiftedPatchTokenization(layers.Layer):\n    def __init__(\n        self,\n        image_size=IMAGE_SIZE,\n        patch_size=PATCH_SIZE,\n        num_patches=NUM_PATCHES,\n        projection_dim=PROJECTION_DIM,\n        vanilla=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.half_patch = patch_size // 2\n        self.flatten_patches = layers.Reshape((num_patches, -1))\n        self.projection = layers.Dense(units=projection_dim)\n        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n\n    def crop_shift_pad(self, images, mode):\n        # Build the diagonally shifted images\n        if mode == \"left-up\":\n            crop_height = self.half_patch\n            crop_width = self.half_patch\n            shift_height = 0\n            shift_width = 0\n        elif mode == \"left-down\":\n            crop_height = 0\n            crop_width = self.half_patch\n            shift_height = self.half_patch\n            shift_width = 0\n        elif mode == \"right-up\":\n            crop_height = self.half_patch\n            crop_width = 0\n            shift_height = 0\n            shift_width = self.half_patch\n        else:\n            crop_height = 0\n            crop_width = 0\n            shift_height = self.half_patch\n            shift_width = self.half_patch\n\n        # Crop the shifted images and pad them\n        crop = tf.image.crop_to_bounding_box(\n            images,\n            offset_height=crop_height,\n            offset_width=crop_width,\n            target_height=self.image_size - self.half_patch,\n            target_width=self.image_size - self.half_patch,\n        )\n        shift_pad = tf.image.pad_to_bounding_box(\n            crop,\n            offset_height=shift_height,\n            offset_width=shift_width,\n            target_height=self.image_size,\n            target_width=self.image_size,\n        )\n        return shift_pad\n\n    def call(self, images):\n        if not self.vanilla:\n            # Concat the shifted images with the original image\n            images = tf.concat(\n                [\n                    images,\n                    self.crop_shift_pad(images, mode=\"left-up\"),\n                    self.crop_shift_pad(images, mode=\"left-down\"),\n                    self.crop_shift_pad(images, mode=\"right-up\"),\n                    self.crop_shift_pad(images, mode=\"right-down\"),\n                ],\n                axis=-1,\n            )\n        # Patchify the images and flatten it\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        flat_patches = self.flatten_patches(patches)\n        if not self.vanilla:\n            # Layer normalize the flat patches and linearly project it\n            tokens = self.layer_norm(flat_patches)\n            tokens = self.projection(tokens)\n        else:\n            # Linearly project the flat patches\n            tokens = self.projection(flat_patches)\n        return (tokens, patches)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.147738Z","iopub.execute_input":"2023-07-10T13:07:35.148076Z","iopub.status.idle":"2023-07-10T13:07:35.166807Z","shell.execute_reply.started":"2023-07-10T13:07:35.148043Z","shell.execute_reply":"2023-07-10T13:07:35.165708Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# BERT Vectors - 768\n","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(\n        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n\n    def call(self, encoded_patches):\n        encoded_positions = self.position_embedding(self.positions)\n        encoded_patches = encoded_patches + encoded_positions\n        return encoded_patches","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.168314Z","iopub.execute_input":"2023-07-10T13:07:35.168934Z","iopub.status.idle":"2023-07-10T13:07:35.177506Z","shell.execute_reply.started":"2023-07-10T13:07:35.168900Z","shell.execute_reply":"2023-07-10T13:07:35.176455Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # The trainable temperature term. The initial value is\n        # the square root of the key dimension.\n        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n\n    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n        query = tf.multiply(query, 1.0 / self.tau)\n        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n        attention_scores_dropout = self._dropout_layer(\n            attention_scores, training=training\n        )\n        attention_output = tf.einsum(\n            self._combine_equation, attention_scores_dropout, value\n        )\n        return attention_output, attention_scores","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.179437Z","iopub.execute_input":"2023-07-10T13:07:35.179815Z","iopub.status.idle":"2023-07-10T13:07:35.189286Z","shell.execute_reply.started":"2023-07-10T13:07:35.179782Z","shell.execute_reply":"2023-07-10T13:07:35.187549Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x\n\n\n# Build the diagonal attention mask\ndiag_attn_mask = 1 - tf.eye(NUM_PATCHES)\ndiag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.191300Z","iopub.execute_input":"2023-07-10T13:07:35.191578Z","iopub.status.idle":"2023-07-10T13:07:35.214297Z","shell.execute_reply.started":"2023-07-10T13:07:35.191554Z","shell.execute_reply":"2023-07-10T13:07:35.213402Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def create_vit_classifier(vanilla=False):\n    inputs = layers.Input(shape=INPUT_SHAPE)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder()(tokens)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(TRANSFORMER_LAYERS):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        if not vanilla:\n            attention_output = MultiHeadAttentionLSA(\n                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n            )(x1, x1, attention_mask=diag_attn_mask)\n        else:\n            attention_output = layers.MultiHeadAttention(\n                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n            )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    # Global average pooling\n    pooled_representation = layers.GlobalAveragePooling1D()(representation)\n    pooled_representation = layers.Dropout(0.5)(pooled_representation)\n    # Add MLP.\n    features = mlp(pooled_representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n    # Output feature vector\n    feature_vector = features\n\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=feature_vector)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.215587Z","iopub.execute_input":"2023-07-10T13:07:35.215998Z","iopub.status.idle":"2023-07-10T13:07:35.226830Z","shell.execute_reply.started":"2023-07-10T13:07:35.215963Z","shell.execute_reply":"2023-07-10T13:07:35.225854Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 2\nINPUT_SHAPE = (72, 72, 3)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.228259Z","iopub.execute_input":"2023-07-10T13:07:35.228958Z","iopub.status.idle":"2023-07-10T13:07:35.238891Z","shell.execute_reply.started":"2023-07-10T13:07:35.228924Z","shell.execute_reply":"2023-07-10T13:07:35.237843Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Some code is taken from:\n# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\n\nclass WarmUpCosine(LearningRateSchedule):\n    def __init__(self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps):\n        super().__init__()\n\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.pi = tf.constant(np.pi)\n\n    def __call__(self, step):\n        if self.total_steps < self.warmup_steps:\n            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n\n        cos_annealed_lr = tf.cos(\n            self.pi * (tf.cast(step, tf.float32) - self.warmup_steps) /\n            float(self.total_steps - self.warmup_steps)\n        )\n        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n\n        if self.warmup_steps > 0:\n            if self.learning_rate_base < self.warmup_learning_rate:\n                raise ValueError(\n                    \"Learning_rate_base must be larger or equal to warmup_learning_rate.\"\n                )\n            slope = (self.learning_rate_base - self.warmup_learning_rate) / self.warmup_steps\n            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n            learning_rate = tf.where(step < self.warmup_steps, warmup_rate, learning_rate)\n\n        return tf.where(step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\")\n\n\n# def run_experiment(model, x_train, y_train, x_test, y_test, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, EPOCHS):\n#     total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n#     warmup_epoch_percentage = 0.10\n#     warmup_steps = int(total_steps * warmup_epoch_percentage)\n#     scheduled_lrs = WarmUpCosine(\n#         learning_rate_base=LEARNING_RATE,\n#         total_steps=total_steps,\n#         warmup_learning_rate=0.0,\n#         warmup_steps=warmup_steps,\n#     )\n\n#     optimizer = tfa.optimizers.AdamW(\n#         learning_rate=scheduled_lrs,\n#         weight_decay=WEIGHT_DECAY\n#     )\n\n#     model.compile(\n#         optimizer=optimizer,\n#         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#         metrics=[\n#             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n#             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n#         ],\n#     )\n\n#     history = model.fit(\n#         x=x_train,\n#         y=y_train,\n#         batch_size=BATCH_SIZE,\n#         epochs=EPOCHS,\n#         validation_split=0.1,\n#     )\n#     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n#     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n#     return history\n\n\n# # # Run experiments with the vanilla ViT\n# # vit = create_vit_classifier(vanilla=True)\n# # history = run_experiment(vit, x_train, y_train, x_test, y_test, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, EPOCHS)\n\n# # Run experiments with the Shifted Patch Tokenization and Locality Self Attention modified ViT\n# vit_sl = create_vit_classifier(vanilla=False)\n# history = run_experiment(vit_sl, x_train, y_train, x_test, y_test, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, EPOCHS)\ndef run_experiment(model, x_train, y_train, x_test, y_test, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, EPOCHS):\n    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n    warmup_epoch_percentage = 0.10\n    warmup_steps = int(total_steps * warmup_epoch_percentage)\n    scheduled_lrs = WarmUpCosine(\n        learning_rate_base=LEARNING_RATE,\n        total_steps=total_steps,\n        warmup_learning_rate=0.0,\n        warmup_steps=warmup_steps,\n    )\n\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=scheduled_lrs,\n        weight_decay=WEIGHT_DECAY\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_split=0.1,\n    )\n    \n    # Get the feature vectors before running\n    before_vectors = model.predict(x_train)\n    \n    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n    \n    # Get the feature vectors after running\n    after_vectors = model.predict(x_train)\n    \n    return history, before_vectors, after_vectors\n\n\n# Create the model\nvit_sl = create_vit_classifier(vanilla=False)\n\n# Run the experiment and get the history, feature vectors before, and feature vectors after\nhistory, before_vectors, after_vectors = run_experiment(vit_sl, x_train, y_train, x_test, y_test, LEARNING_RATE, WEIGHT_DECAY, BATCH_SIZE, EPOCHS)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:07:35.240659Z","iopub.execute_input":"2023-07-10T13:07:35.241134Z","iopub.status.idle":"2023-07-10T13:08:37.618132Z","shell.execute_reply.started":"2023-07-10T13:07:35.241100Z","shell.execute_reply":"2023-07-10T13:08:37.617091Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1/10\n9/9 [==============================] - 17s 568ms/step - loss: 6.4358 - accuracy: 0.1826 - top-5-accuracy: 0.2768 - val_loss: 2.8017 - val_accuracy: 1.0000 - val_top-5-accuracy: 1.0000\nEpoch 2/10\n9/9 [==============================] - 3s 367ms/step - loss: 4.3196 - accuracy: 0.4560 - top-5-accuracy: 0.9483 - val_loss: 3.7950 - val_accuracy: 0.6217 - val_top-5-accuracy: 1.0000\nEpoch 3/10\n9/9 [==============================] - 3s 368ms/step - loss: 4.1934 - accuracy: 0.4681 - top-5-accuracy: 0.9957 - val_loss: 3.0972 - val_accuracy: 0.9957 - val_top-5-accuracy: 1.0000\nEpoch 4/10\n9/9 [==============================] - 3s 374ms/step - loss: 4.2155 - accuracy: 0.4222 - top-5-accuracy: 0.9227 - val_loss: 3.1638 - val_accuracy: 1.0000 - val_top-5-accuracy: 1.0000\nEpoch 5/10\n9/9 [==============================] - 3s 379ms/step - loss: 4.0771 - accuracy: 0.4807 - top-5-accuracy: 0.9928 - val_loss: 3.2289 - val_accuracy: 0.0783 - val_top-5-accuracy: 1.0000\nEpoch 6/10\n9/9 [==============================] - 3s 383ms/step - loss: 4.0238 - accuracy: 0.4903 - top-5-accuracy: 0.9966 - val_loss: 3.2627 - val_accuracy: 1.0000 - val_top-5-accuracy: 1.0000\nEpoch 7/10\n9/9 [==============================] - 4s 392ms/step - loss: 4.0492 - accuracy: 0.4744 - top-5-accuracy: 0.9937 - val_loss: 3.2395 - val_accuracy: 0.9522 - val_top-5-accuracy: 1.0000\nEpoch 8/10\n9/9 [==============================] - 4s 389ms/step - loss: 4.0860 - accuracy: 0.4594 - top-5-accuracy: 0.9990 - val_loss: 3.2301 - val_accuracy: 0.6522 - val_top-5-accuracy: 1.0000\nEpoch 9/10\n9/9 [==============================] - 3s 381ms/step - loss: 4.1529 - accuracy: 0.4778 - top-5-accuracy: 0.9986 - val_loss: 3.3129 - val_accuracy: 0.0043 - val_top-5-accuracy: 1.0000\nEpoch 10/10\n9/9 [==============================] - 3s 385ms/step - loss: 4.0397 - accuracy: 0.4860 - top-5-accuracy: 0.9981 - val_loss: 3.3254 - val_accuracy: 0.0043 - val_top-5-accuracy: 1.0000\n72/72 [==============================] - 3s 27ms/step\n3/3 [==============================] - 0s 124ms/step - loss: 3.2849 - accuracy: 0.5922 - top-5-accuracy: 1.0000\nTest accuracy: 59.22%\nTest top 5 accuracy: 100.0%\n72/72 [==============================] - 2s 25ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the shape of the feature vectors\nprint(\"Shape of feature vectors:\", before_vectors.shape)\n\n# Print the length of the feature vectors\nprint(\"Length of feature vectors:\", len(before_vectors))\n\n# Print the first 10 values of the feature vectors\nprint(\"First 1 values of feature vectors:\")\nprint(before_vectors[:1])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:08:37.619992Z","iopub.execute_input":"2023-07-10T13:08:37.620694Z","iopub.status.idle":"2023-07-10T13:08:37.629122Z","shell.execute_reply.started":"2023-07-10T13:08:37.620658Z","shell.execute_reply":"2023-07-10T13:08:37.627786Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Shape of feature vectors: (2300, 1024)\nLength of feature vectors: 2300\nFirst 1 values of feature vectors:\n[[ 3.5828161   3.51928    -0.16993013 ... -0.16995241 -0.16986436\n  -0.16905013]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the shape of the feature vectors\nprint(\"Shape of feature vectors:\", after_vectors.shape)\n\n# Print the length of the feature vectors\nprint(\"Length of feature vectors:\", len(after_vectors))\n\n# Print the first 10 values of the feature vectors\nprint(\"First 1 values of feature vectors:\")\nprint(after_vectors[:1])","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:08:37.631379Z","iopub.execute_input":"2023-07-10T13:08:37.632132Z","iopub.status.idle":"2023-07-10T13:08:37.642061Z","shell.execute_reply.started":"2023-07-10T13:08:37.632092Z","shell.execute_reply":"2023-07-10T13:08:37.640619Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Shape of feature vectors: (2300, 1024)\nLength of feature vectors: 2300\nFirst 1 values of feature vectors:\n[[ 3.5828161   3.51928    -0.16993013 ... -0.16995241 -0.16986436\n  -0.16905013]]\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Access the features tensor\nfeatures = vit_sl.layers[-2].output\n\n# Check the size of the feature vector\nfeature_vector_size = features.shape\nprint(\"Size of the feature vector:\", feature_vector_size)\n# Assuming `feature` is the variable you want to check\n\nif isinstance(features, list):\n    print(\"The features is a list.\")\nelif isinstance(features, np.ndarray):\n    print(\"The features is an array.\")\nelse:\n    print(\"The features is an object.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:08:37.644676Z","iopub.execute_input":"2023-07-10T13:08:37.645361Z","iopub.status.idle":"2023-07-10T13:08:37.653485Z","shell.execute_reply.started":"2023-07-10T13:08:37.645323Z","shell.execute_reply":"2023-07-10T13:08:37.652251Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Size of the feature vector: (None, 1024)\nThe features is an object.\n","output_type":"stream"}]},{"cell_type":"code","source":"len(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:21:27.749807Z","iopub.execute_input":"2023-07-10T13:21:27.750257Z","iopub.status.idle":"2023-07-10T13:21:27.759877Z","shell.execute_reply.started":"2023-07-10T13:21:27.750211Z","shell.execute_reply":"2023-07-10T13:21:27.758884Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"667"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, concatenate\n\n\n\n# Define input layers for image and text features\nimage_input = Input(shape=(1024,))\ntext_input = Input(shape=(1024,))\n\n# Concatenate the input layers\nconcatenated = concatenate([image_input, text_input])\n\n# Add fully connected layers for classification\ndense1 = Dense(512, activation='relu')(concatenated)\ndense2 = Dense(256, activation='relu')(dense1)\noutput = Dense(1, activation='sigmoid')(dense2)\n\n# Define the multimodal model\nmodel = Model(inputs=[image_input, text_input], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the multimodal model\n\nmodel.fit([after_vectors, bert2], y_train, epochs=10, batch_size=32)\n\n# # Test the multimodal model\n# test_loss, test_acc = model.evaluate([x_test], y_test)\n# print('Test accuracy:', test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:52:10.695882Z","iopub.execute_input":"2023-07-10T13:52:10.696284Z","iopub.status.idle":"2023-07-10T13:52:16.556169Z","shell.execute_reply.started":"2023-07-10T13:52:10.696252Z","shell.execute_reply":"2023-07-10T13:52:16.555043Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch 1/10\n72/72 [==============================] - 2s 4ms/step - loss: 0.4067 - accuracy: 0.8152\nEpoch 2/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3846 - accuracy: 0.8339\nEpoch 3/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3752 - accuracy: 0.8374\nEpoch 4/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8357\nEpoch 5/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8409\nEpoch 6/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3666 - accuracy: 0.8378\nEpoch 7/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3844 - accuracy: 0.8370\nEpoch 8/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8422\nEpoch 9/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3768 - accuracy: 0.8370\nEpoch 10/10\n72/72 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8383\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7941e44d8040>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\n# Get the feature vectors for images and text\n# image_feature_vectors = # Shape: (2300, 1024)\n# text_feature_vectors = # Shape: (2300, 1024)\n\n# Select the relevant samples for testing\nimage_test_feature_vectors = after_vectors[:667]\ntext_test_feature_vectors = bert_test[:667]\n\n# Evaluate the multimodal model\ntest_loss, test_acc = model.evaluate({'image_inputs': image_test_feature_vectors, 'text_inputs': text_test_feature_vectors}, y_test)\n\nprint('Test accuracy:', test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:52:18.035580Z","iopub.execute_input":"2023-07-10T13:52:18.035945Z","iopub.status.idle":"2023-07-10T13:52:18.173224Z","shell.execute_reply.started":"2023-07-10T13:52:18.035916Z","shell.execute_reply":"2023-07-10T13:52:18.171890Z"},"trusted":true},"execution_count":73,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[73], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m text_test_feature_vectors \u001b[38;5;241m=\u001b[39m bert_test[:\u001b[38;5;241m667\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate the multimodal model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_inputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_test_feature_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_inputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_test_feature_vectors\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_file3iyxic5z.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_10\". You passed a data dictionary with keys ['image_inputs', 'text_inputs']. Expected the following keys: ['input_10', 'input_11']\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_10\". You passed a data dictionary with keys ['image_inputs', 'text_inputs']. Expected the following keys: ['input_10', 'input_11']\n","output_type":"error"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Image feature vector shape: (2300, 1024)\n# Text feature vector shape: (2300, 1024)\n# Test word embedding shape: (667, 1024)\n\n# Define the multimodal model\nimage_inputs = keras.Input(shape=(1024,), name='image_inputs')\ntext_inputs = keras.Input(shape=(1024,), name='text_inputs')\n\n# Concatenate the image and text feature vectors\nconcatenated = layers.concatenate([image_inputs, text_inputs])\n\n# Add more layers for multimodal classification\ndense1 = layers.Dense(256, activation='relu')(concatenated)\ndense2 = layers.Dense(128, activation='relu')(dense1)\ndropout1 = layers.Dropout(0.1)(dense2)\ndense3 = layers.Dense(64, activation='relu')(dropout1)\ndropout2 = layers.Dropout(0.2)(dense3)\noutputs = layers.Dense(1, activation='softmax')(dropout2)\n\n# Create the multimodal model\nmodel = keras.Model(inputs=[image_inputs, text_inputs], outputs=outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the multimodal model\nhistory = model.fit({'image_inputs': after_vectors, 'text_inputs': bert2}, y_train, epochs=10, batch_size=32)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:34:44.530962Z","iopub.execute_input":"2023-07-10T13:34:44.531373Z","iopub.status.idle":"2023-07-10T13:34:50.639302Z","shell.execute_reply.started":"2023-07-10T13:34:44.531337Z","shell.execute_reply":"2023-07-10T13:34:50.638171Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Epoch 1/10\n72/72 [==============================] - 2s 4ms/step - loss: 0.4132 - accuracy: 0.5574\nEpoch 2/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.5574\nEpoch 3/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3802 - accuracy: 0.5574\nEpoch 4/10\n72/72 [==============================] - 0s 5ms/step - loss: 0.3804 - accuracy: 0.5574\nEpoch 5/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.5574\nEpoch 6/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.5574\nEpoch 7/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3833 - accuracy: 0.5574\nEpoch 8/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.5574\nEpoch 9/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3823 - accuracy: 0.5574\nEpoch 10/10\n72/72 [==============================] - 0s 4ms/step - loss: 0.3697 - accuracy: 0.5574\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Get the feature vectors for images and text\n# image_feature_vectors = # Shape: (2300, 1024)\n# text_feature_vectors = # Shape: (2300, 1024)\n\n# Select the relevant samples for testing\nimage_test_feature_vectors = after_vectors[:667]\ntext_test_feature_vectors = bert_test[:667]\n\n# Evaluate the multimodal model\ntest_loss, test_acc = model.evaluate({'image_inputs': image_test_feature_vectors, 'text_inputs': text_test_feature_vectors}, y_test)\n\nprint('Test accuracy:', test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:51:12.011281Z","iopub.execute_input":"2023-07-10T13:51:12.011736Z","iopub.status.idle":"2023-07-10T13:51:12.366242Z","shell.execute_reply.started":"2023-07-10T13:51:12.011697Z","shell.execute_reply":"2023-07-10T13:51:12.364909Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"21/21 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.4078\nTest accuracy: 0.40779611468315125\n","output_type":"stream"}]}]}